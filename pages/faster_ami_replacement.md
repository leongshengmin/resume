# AMI Replacement w/o Replacing Instance

Currently we terminate the entire instance just to replace the AMI (takes a few hours especially for stateful nodes since we're draining the data prior termination).

There’s an AWS feature that allows swapping out the root disk (which stores the AMI) that will still allow us to retain data in instance store / attached EBS, just requires a host reboot. This reduces the AMI replacement down to a few minutes.

**Caveats:**

- root volume snapshot needs to be shared to the subaccount for the subaccount to have permissions to replace root disk.
- all processes on host will be stopped while root volume is swapped since `reboot` is run.
- for raid array setup, need to update ansible-init playbook to get raid device name instead of hardcoding this (as raid autodetect is by default enabled so upon reboot, raid device name may change from `/dev/md0` → `/dev/md/<hostname>` depending on the autogenerated `/etc/mdadm/mdadm.conf` file rules).

[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/replace-root.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/replace-root.html)


### Testing on Stateful Opensearch Data Node

**Verification steps:**
```/bin/sh
- [OK] create test files using urandom in /storage where raid device is mounted and calculate checksum of file contents so we know if they're modified after reboot
- [OK] ensure mdadm --detail --scan shows raid0 array registered
- [OK] ensure lsblk shows raid0 array mounted onto /storage
- [OK] ensure ansible-init no errors
- [OK] ensure opensearch running and is able to rejoin the cluster
```


**Before ami replacement:**

Get checksum of urandom files created in /storage dir where raid device is mounted
```
/bin/sh
dd if=/dev/urandom of=/storage/opensearch/testurandombytes_1M_10 bs=1M count=10
dd if=/dev/urandom of=/storage/opensearch/testurandombytes_1M_80 bs=1M count=80

md5sum /storage/opensearch/testurandombytes_1M_*
4d0956672a5f86718a9a10386a65603c  /storage/opensearch/testurandombytes_1M_10
64da9dc7ea78a2fc236bbd3cbf0c9238  /storage/opensearch/testurandombytes_1M_80
```

Current state of `lsblk` + raid array setup from `mdadm --detail --scan`

```/bin/sh
lsblk
NAME         MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
loop0          7:0    0 21.9M  1 loop  /snap/amazon-ssm-agent/xx
loop1          7:1    0 49.1M  1 loop  /snap/core18/...
loop2          7:2    0   92M  1 loop  /snap/lxd/...
loop3          7:3    0 59.8M  1 loop  /snap/core20/...
loop4          7:4    0 33.7M  1 loop  /snap/snapd/...
nvme0n1      259:0    0    8G  0 disk
├─nvme0n1p1  259:3    0  7.9G  0 part  /
└─nvme0n1p15 259:4    0   99M  0 part  /boot/efi
nvme2n1      259:1    0   10G  0 disk
└─md0          9:0    0   20G  0 raid0 /storage
nvme1n1      259:2    0   10G  0 disk
└─md0          9:0    0   20G  0 raid0 /storage

root@ip-x-x-x-x:/var/snap/amazon-ssm-agent/xx# mdadm --detail --scan
ARRAY /dev/md0 metadata=1.2 name=ip-x-x-x-x:opensearch UUID=df0d129d:3ba8cd00:b574d474:e596f551
```

Ansible init output

```/bin/sh
PLAY RECAP ****************************************************************************************************************************************************************************************************************************************************************************
localhost                  : ok=63   changed=29   unreachable=0    failed=0    skipped=41   rescued=0    ignored=2
```

**After ami replacement:**

Verify Ansible init output & verify that opensearch is running and able to rejoin cluster

```/bin/sh
PLAY RECAP *********************************************************************
localhost                  : ok=67   changed=35   unreachable=0    failed=0    skipped=43   rescued=0    ignored=2
```

```
root@ip-x-x-x-x:/var/snap/amazon-ssm-agent/xx# systemctl status opensearch
● opensearch.service - opensearch
     Loaded: loaded (/etc/systemd/system/opensearch.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/opensearch.service.d
             └─override.conf
     Active: active (running) since Wed 2024-12-18 09:47:48 UTC; 22s ago
   Main PID: 4518 (java)
      Tasks: 24 (limit: 18920)
     Memory: 9.8G
     CGroup: /system.slice/opensearch.service
             └─4518 /usr/share/opensearch/jdk/bin/java -Xshare:auto -Dopensearch.networkaddress.cache.ttl=60 -Dopensearch.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThr>
```

Current state of `lsblk` + raid array setup from `mdadm --detail --scan` show that checksums are the same.

```/bin/sh
lsblk
NAME         MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
loop0          7:0    0 21.9M  1 loop  /snap/amazon-ssm-agent/xx
loop1          7:1    0 49.1M  1 loop  /snap/core18/...
loop2          7:2    0   92M  1 loop  /snap/lxd/...
loop3          7:3    0 59.8M  1 loop  /snap/core20/...
loop4          7:4    0 33.7M  1 loop  /snap/snapd/...
loop5          7:5    0 48.8M  1 loop  /snap/core18/...
loop6          7:6    0 59.5M  1 loop  /snap/core20/...
nvme0n1      259:0    0    8G  0 disk
├─nvme0n1p1  259:3    0  7.9G  0 part  /
└─nvme0n1p15 259:4    0   99M  0 part  /boot/efi
nvme2n1      259:1    0   10G  0 disk
└─md127        9:127  0   20G  0 raid0 /storage
nvme1n1      259:2    0   10G  0 disk
└─md127        9:127  0   20G  0 raid0 /storage

root@ip-x-x-x-x:/var/snap/amazon-ssm-agent/xx# mdadm --detail --scan
ARRAY /dev/md/ip-x-x-x-x:opensearch UUID=df0d129d:3ba8cd00:b574d474:e596f551
root@ip-x-x-x-x:/var/snap/amazon-ssm-agent/xx# md5sum /storage/opensearch/testurandombytes_1M_*
4d0956672a5f86718a9a10386a65603c  /storage/opensearch/testurandombytes_1M_10
64da9dc7ea78a2fc236bbd3cbf0c9238  /storage/opensearch/testurandombytes_1M_80
```

dmesg log
```/bin/sh
dmesg --human --time-format iso --nopager
...
2024-12-18T09:46:50,454727+00:00 md: Waiting for all devices to be available before autodetect
2024-12-18T09:46:50,455703+00:00 md: If you don't use raid, use raid=noautodetect
2024-12-18T09:46:50,456515+00:00 md: Autodetecting RAID arrays. <----------------------------------------------- Since we're autodetecting raid arrays here instead of updating the mdadm.conf + updating initramfs, raid device may change after reboot from e.g /dev/md0 to /dev/md/<hostname>

2024-12-18T09:46:50,457109+00:00 md: autorun ...
2024-12-18T09:46:50,457522+00:00 md: ... autorun DONE.
2024-12-18T09:46:50,462427+00:00 EXT4-fs (nvme0n1p1): mounted filesystem with ordered data mode. Opts: (null). Quota mode: none.
2024-12-18T09:46:50,463839+00:00 VFS: Mounted root (ext4 filesystem) readonly on device 259:3.
2024-12-18T09:46:50,465355+00:00 devtmpfs: mounted
2024-12-18T09:46:50,468402+00:00 Freeing unused kernel memory: 9536K
2024-12-18T09:46:50,502834+00:00 Checked W+X mappings: passed, no W+X pages found
2024-12-18T09:46:50,503664+00:00 Run /sbin/init as init process
2024-12-18T09:46:50,504257+00:00   with arguments:
2024-12-18T09:46:50,504258+00:00     /sbin/init
2024-12-18T09:46:50,504259+00:00   with environment:
2024-12-18T09:46:50,504260+00:00     HOME=/
2024-12-18T09:46:50,504261+00:00     TERM=linux
2024-12-18T09:46:50,504262+00:00     BOOT_IMAGE=/boot/vmlinuz-5.15.0-1070-aws <-------------------------------------------- Boot image loaded

2024-12-18T09:46:50,725555+00:00 systemd[1]: Inserted module 'autofs4'
2024-12-18T09:46:50,757639+00:00 systemd[1]: systemd 245.4-4ubuntu3.24 running in system mode. (+PAM +AUDIT +SELINUX +IMA +APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy=hybrid)
2024-12-18T09:46:50,760951+00:00 systemd[1]: Detected architecture arm64.
2024-12-18T09:46:50,794473+00:00 systemd[1]: Set hostname to <ip-x-x-x-x>.
...
2024-12-18T09:47:33,362958+00:00 XFS (md127): Mounting V5 Filesystem
2024-12-18T09:47:33,452761+00:00 XFS (md127): Ending clean mount
2024-12-18T09:47:33,479344+00:00 xfs filesystem being mounted at /storage supports timestamps until 2038 (0x7fffffff) <--------------------------------------------- FS mounted successfully

```
